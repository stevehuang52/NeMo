{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc99d2f-9ac6-40c2-b072-12b6cb7b9aca",
   "metadata": {},
   "source": [
    "### Step 3: Prune the fine-tuned teacher model to create a student\n",
    "In this step, we will explore two methods to prune the fine-tuned teacher model. Refer to the ``NOTE`` in the **_step-by-step instructions_** section of [introduction.ipynb](./introduction.ipynb) to decide which pruning techniques you would like to explore.\n",
    "\n",
    "In the first method, depth-pruning, we trim the layers of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa494e-6268-4044-a1d6-c0518d450cfd",
   "metadata": {},
   "source": [
    "#### Step 3.a.: Using depth-pruning \n",
    "To depth-prune, we will trim the layers 16-31 (leaving 1-15 and 32) in the finetined teacher model. For depth-pruning, we would be using the [megatron_gpt_prune](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_prune.py) script. \n",
    "\n",
    "Per the [blog](https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/) and [tech report](https://arxiv.org/pdf/2408.11796), removing contiguous layers from the second last block (layers 16 to 31 continuously) yields the best overall results. \n",
    "\n",
    "> `NOTE:` In the block of code below, pass the paths to your fine-tuned teacher .nemo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cae073-a192-4d47-b220-b09736d39a93",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!torchrun --nproc_per_node=8 \\\n",
    "     /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_prune.py \\\n",
    "     model.restore_from_path=\"./distill_trainings/megatron_llama_ft/checkpoints/megatron_llama_ft.nemo\" \\\n",
    "     model.tensor_model_parallel_size=8 \\\n",
    "     model.pipeline_model_parallel_size=1 \\\n",
    "     +model.dist_ckpt_load_strictness=log_all \\\n",
    "     trainer.num_nodes=1 \\\n",
    "     trainer.precision=bf16 \\\n",
    "     trainer.devices=8 \\\n",
    "     \"prune.drop_layers=[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\" \\\n",
    "     export.save_path=\"/workspace/4b_depth_pruned_model.nemo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f298a-0363-4f44-b40c-2c8e9bab7d76",
   "metadata": {},
   "source": [
    "Running this script will save the depth-pruned model `4b_depth_pruned_model.nemo` to your workspace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
