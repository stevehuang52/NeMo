{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc99d2f-9ac6-40c2-b072-12b6cb7b9aca",
   "metadata": {},
   "source": [
    "### Step 3: Prune the fine-tuned teacher model to create a student\n",
    "In the second method, we will width-prune. In width-pruning, we trim the neurons, attention heads, and embedding channels.\n",
    "\n",
    "Refer to the ``NOTE`` in the **_step-by-step instructions_** section of [introduction.ipynb](./introduction.ipynb) to decide which pruning techniques you would like to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207ed14-2f37-4712-88f3-543a128663ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 3.b.: Using width-pruning\n",
    "To width-prune the model, we do the following:\n",
    "- Prune (trim) the MLP intermediate dimension from 14336 to 9216.\n",
    "- Prune the hidden size from 4096 to 3072.\n",
    "- Retrain the attention headcount and number of layers\n",
    "\n",
    "For width-pruning, we will use the [megatron_gpt_prune.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_prune.py) script in the NeMo Framework. To see the detailed list of parameters for width-pruning, you can view the [megatron_gpt_prune.yaml](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/conf/megatron_gpt_prune.yaml) file.\n",
    "\n",
    "We use the above parameters to get a competitive model for this demonstration. You can use other strategies or parameters from the [blog](https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/) or the [tech report](https://arxiv.org/pdf/2408.11796) for your experiments. \n",
    "\n",
    "> `NOTE:`  In the block of code below, pass the paths to your fine-tuned teacher .nemo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d1483-dd4c-403e-b321-293342e7a62a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!torchrun --nproc-per-node=8 /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_prune.py \\\n",
    "     model.restore_from_path=\"./distill_trainings/megatron_llama_ft/checkpoints/megatron_llama_ft.nemo\" \\\n",
    "     model.tensor_model_parallel_size=1 \\\n",
    "     model.pipeline_model_parallel_size=8 \\\n",
    "     +model.dist_ckpt_load_strictness=log_all \\\n",
    "     inference.batch_size=1 \\\n",
    "     trainer.num_nodes=1 \\\n",
    "     trainer.precision=bf16 \\\n",
    "     trainer.devices=8 \\\n",
    "     prune.ffn_hidden_size=9216 \\\n",
    "     prune.num_attention_heads=null \\\n",
    "     prune.num_query_groups=null \\\n",
    "     prune.hidden_size=3072 \\\n",
    "     export.save_path=\"/workspace/4b_width_pruned_model.nemo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb0977-5c02-4ecc-b602-54d74b2e2184",
   "metadata": {},
   "source": [
    "Running this script will save the width-pruned model `4b_width_pruned_model.nemo` to your workspace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
